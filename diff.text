diff --git a/README.md b/README.md
index 43885db..45d4e8b 100644
--- a/README.md
+++ b/README.md
@@ -41,3 +41,6 @@ https://docs.aws.amazon.com/sdk-for-rust/latest/dg/using.html
 https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html
 https://github.com/awslabs/aws-sdk-rust/tree/main/examples/examples/s3
 https://docs.rs/aws-sdk-s3/latest/aws_sdk_s3/index.html
+
+
+https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html
diff --git a/default.config b/default.config
new file mode 100644
index 0000000..4eeac2f
--- /dev/null
+++ b/default.config
@@ -0,0 +1,12 @@
+
+# Server address to connect to:
+server_address 0.0.0.0
+
+# Server port to use:
+server_port 44113
+
+# Path to log file:
+log_paths test1.log test2.log /var/log/logminer/logs/crontest.log
+
+# Path to TLS Credentials:
+credentials path/to/creds
diff --git a/src/awss3.rs b/src/awss3.rs
index 0a1e69e..852c426 100644
--- a/src/awss3.rs
+++ b/src/awss3.rs
@@ -1,9 +1,12 @@
 use aws_config::meta::region::RegionProviderChain;
-
-use aws_sdk_s3::{config::Region, meta::PKG_VERSION, Client};
-
+use aws_sdk_s3::{config::Region, meta::PKG_VERSION, Client, Error};
 use aws_sdk_s3::primitives::ByteStream;
-use s3_service::error::Error;
+use std::path::Path;
+use std::process;
+use std::sync::Arc;
+use clap::Parser;
+
+//use s3_service::error::Error;
 
 /*
 use clap::Parser;
@@ -30,35 +33,46 @@ pub struct Opt {
 pub fn 
 upload_object(
     data: &str,
-    client: &Client,
-    bucket: &str,
-    key: &str,
+    client: Client,
+    //bucket: &str,
 ) -> Result<(), Error> {
     // this needs to append to the existing log data object, not overwrite it.
-    let body = ByteStream::from_static("hello world this is a test".as_bytes());
-    let _resp = client
+    //let body = ByteStream::from_static(data.as_bytes());
+    println!("data: {data:?}");
+    let resp = &client
         .put_object()
-        .bucket(bucket)
-        .key(key)
-        .body(body)
+        .bucket("logcollectionbucket")
+        .key(data)
+        //.body(body)
         .send();
-        //.await?; // is this needed?
+
+    //println!("Upload success. Version: {:?}", resp.version_id);
+
+    //let resp = client.get_object()
+    //            .bucket("logcollectionbucket").key(data).send().await?;
+    //let d = resp.body.collect().await;
+    //println!("data: {:?}", d.unwrap().into_bytes());
     Ok(())
 }
 
 
-// Client::new is an expesive operation. Decompose this function as well as 
-// upload_object so that it takes the log data in the producer function.
-#[tokio::main]
 pub async fn 
 start_s3() -> Result<Client, Error> {
-
     //https://docs.rs/aws-config/latest/aws_config/index.html
     let region_provider = RegionProviderChain::default_provider()
         .or_else(Region::new("us-west-2"));
 
-    println!();
     let shared_config = aws_config::from_env().region(region_provider).load().await;
+    let client = Client::new(&shared_config); 
+
+    //// Test upload event object
+    //let bucket = String::from("logcollectionbucket");
+    //let data = String::from("another test before stopping");
+    //let result = upload_object(&client,&bucket,&data).await;
+    //match result {
+    //    Ok(val) => println!("{val:?}"),
+    //    Err(err) => eprintln!("{err:?}"),
+    //}
 
-    Ok(Client::new(&shared_config))
+    Ok(client)
 }
diff --git a/src/config.rs b/src/config.rs
index f3eb834..cdc42fe 100644
--- a/src/config.rs
+++ b/src/config.rs
@@ -34,23 +34,6 @@ pub struct Config {
 */
 ////end of pita////
 
-//custome error type
-#[derive(Debug)]
-pub enum ConfigError {
-    FileReadError,
-    ParseError,
-}
-
-
-impl std::fmt::Display for ConfigError {
-    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
-        write!(f, "{:?}", self)
-    }
-}
-
-impl std::error::Error for ConfigError {}
-
-
 
 #[derive(Debug)]
 pub struct Config {
@@ -61,8 +44,9 @@ pub struct Config {
     pub credentials: String // TLS needed
 }
 
-/*pub fn
-read_config(client: Client) -> Option<Config> {
+pub fn
+read_config(client: &Client) -> Option<Config> {
+//read_config() -> Option<Config> {
     let mut fields: Vec<String> = Vec::new();
     let file = File::open("test.config").ok()?;
     let reader = BufReader::new(file);
@@ -78,66 +62,21 @@ read_config(client: Client) -> Option<Config> {
             None => continue
         }
     }
-    Some(set_configuration(fields,client))
-}  */
-
-pub fn read_config(client: Client) -> Result<Config, ConfigError> {
-    let file = File::open("test.config").map_err(|_| ConfigError::FileReadError)?;
-    let reader = BufReader::new(file);
-    let mut fields = Vec::new();
-
-    for line in reader.lines() {
-        let line = line.map_err(|_| ConfigError::ParseError)?;
-        if !line.starts_with('#') {
-            fields.push(line);
-        }
-    }
-
-    set_configuration(fields, client)
+    Some(set_configuration(fields,&client))
 }
 
-
-
-
 // If a item in the configuration file is missing,
 // return a default config and let it error out later.
 // TODO: handle the error
 fn
-set_configuration(list: Vec<String>, client: Client) -> Result<Config, ConfigError> {
-    
-    let mut config = Config {
-        server_address: String::from(""),
-        server_port: String::from(""),
-        log_paths: Vec::new(),
-        credentials: String::from(""),
-        s3_client: Some(client),
-    };
-
-    for item in list {
-        let parts: Vec<&str> = item.splitn(2, ' ').collect();
-        if parts.len() != 2 {
-            return Err(ConfigError::ParseError);
-        }
-
-        match parts[0] {
-            "server_address" => config.server_address = parts[1].to_string(),
-            "server_port" => config.server_port = parts[1].to_string(),
-            "log_paths" => config.log_paths = parts[1].split_whitespace().map(String::from).collect(),
-            "credentials" => config.credentials = parts[1].to_string(),
-            _ => return Err(ConfigError::ParseError),
-        }
-    }
-
-    Ok(config)
-}
-/*set_configuration(list: Vec<String>, client: Client) -> Config {
+set_configuration(list: Vec<String>, client: &Client) -> Config {
     // maybe convert these to &str later on.
     let mut config = Config {
         server_address: String::from(""),
         server_port: String::from(""),
         log_paths: Vec::new(),
         credentials: String::from(""),
-        s3_client: Some(client),
+        s3_client: Some(client.clone()),
     };
 
     // TODO: clean this up somehow. just make it work for now.
@@ -169,5 +108,5 @@ set_configuration(list: Vec<String>, client: Client) -> Result<Config, ConfigErr
         }
     }   
     //println!("config: {:?}",config);// long printout with s3_client
-    config 
-} */
+    config
+}
diff --git a/src/main.rs b/src/main.rs
index 32e7c96..f2e45ea 100644
--- a/src/main.rs
+++ b/src/main.rs
@@ -25,9 +25,12 @@ struct Opt {
 
 // still need this to create the event and send to sink. The sink is the s3 bucket
 // see producer:handle_log_data
-fn 
+#[tokio::main]
+async fn 
 main() {
 
+    // these should be optional. for now, hard code them and test
+    /*
     let Opt {
         bucket,
         key,
@@ -43,19 +46,17 @@ main() {
         println!("Log Data Key:               {}", &key);
         println!();
     }
+    */
+    //let result = start_s3();
 
-    let config_data;
-
-    if let Ok(client) = start_s3() {
+    if let Ok(client) = start_s3().await {
         // pass client into configuration 
-        config_data = read_config(client);
+        let config_data = read_config(&client);
         match config_data {
             Some(config) => {
-                let _ = start_log_stream(config);
+                let _ = start_log_stream(config).await;
             }
             None => panic!("error reading configuration. fix it.")
         }
     }
-
-
 }
diff --git a/src/producer.rs b/src/producer.rs
index 6d4f11b..a1c7f3a 100644
--- a/src/producer.rs
+++ b/src/producer.rs
@@ -1,26 +1,13 @@
 use std::process::{Command, Stdio};
 use std::io::{BufReader, BufRead, Result};
 use std::thread;
+use std::sync::Arc;
 use std::sync::mpsc::{channel,Sender,Receiver};
 //use std::net::{TcpStream, SocketAddr};
 use crate::config::{Config};
 use aws_sdk_s3::{Client};
 use crate::awss3;
 
-
-trait LogSink {
-    fn handle_log(&self, log_line: String);
-}
-
-struct StdoutSink;
-
-impl LogSink for StdoutSink {
-    fn handle_log(&self, log_line: String) {
-        println!("{}", log_line);
-    }
-}
-
-
 fn 
 tail_and_send_log(path: &str, sender: Sender<String>) -> Result<()> {
     let mut tail_process = Command::new("tail")
@@ -35,6 +22,7 @@ tail_and_send_log(path: &str, sender: Sender<String>) -> Result<()> {
         for line in reader.lines() {
             if let Ok(line) = line {
                 sender.send(line).expect("Failed to send data");
+
             }
         }
     });
@@ -46,52 +34,58 @@ tail_and_send_log(path: &str, sender: Sender<String>) -> Result<()> {
 // TODO: This function should take a channel and a sink.
 // The sink is the destination for the log data. For now, it just prints to 
 // stdout.
-//fn 
-//handle_log_data(log_channel: Receiver<String>) {
-fn
-handle_log_data<T: LogSink + Send + 'static>(log_channel: Receiver<String>, log_sink: T) {
+fn 
+handle_log_data(log_channel: Receiver<String>, client: Client) {
+    println!("client called");
     for log_line in log_channel {
         // rethink how to provide client, bucket, and key to this call. 
-        //awss3::upload_object(&log_line, &client, "endepointe", "output.txt");
-
-        log_sink.handle_log(&log_line);
-
-        //println!("{}", log_line);
+        println!("{}", log_line);
+        awss3::upload_object(&log_line, client.clone());
     }
 }
 
 // for now, pass in the client,bucket,and key for handle_log_data.
-pub fn 
+pub async fn 
 start_log_stream(config: Config) -> Result<()> {
 
     let mut senders = Vec::new();
     let mut receivers = Vec::new();
+    let mut clients = Vec::<Client>::new();
 
     for input_log_file in config.log_paths.clone().into_iter() {
+        if let Ok(client) = awss3::start_s3().await {
+            clients.push(client);
+        }
         let (sender, receiver) = channel();
         senders.push(sender);
         receivers.push(receiver);
-
+         
         let sender_clone = senders.last().unwrap().clone();
         thread::spawn(move || {
+            // might need Arc for client
             tail_and_send_log(&input_log_file, sender_clone)
                 .expect("Failed to tail log file");
         });
     }
 
-    let log_sink = StdoutSink;
-
-    for receiver in receivers {
-        let log_sink_clone = log_sink.clone(); 
-        thread::spawn(move || handle_log_data(receiver, log_sink_clone));
+    //if let Some(client) = config.s3_client {
+    //    //println!("{client:?}");
+    //    for (receiver, _input_log_file) in receivers.into_iter()
+    //            .zip(config.log_paths.clone()) {
+    //        thread::spawn(move || handle_log_data(receiver));
+    //    }    
+    //}
+
+    let mut count: u8 = 0;
+    for (receiver, client) in receivers.into_iter().zip(clients) {
+        count += 1;
+        println!("called {count}");
+        thread::spawn(move || handle_log_data(receiver, client));
     }
-
-
-    /*if let Some(client) = config.s3_client {
-        for (receiver, _input_log_file) in receivers.into_iter().zip(config.log_paths.clone()) {
-            thread::spawn(move || handle_log_data(receiver));
-        }       
-    }*/
+    //for (receiver, _input_log_file) in receivers.into_iter()
+    //        .zip(config.log_paths.clone()) {
+    //    thread::spawn(move || handle_log_data(receiver));
+    //}       
 
 
     // never return
